Mutual Information

	Correlation
	Pearson correlation not suitable for quadratics and non linear relations
	
	Requires discrete data
		continuous data need to be discretised 
			Put in ranges/ catergories etc
			
			Equal width bins
				Divide range with number of bins to find bin range (width)
			Equal frequency bins
				Number of values in the list divided by number of bins roughly
				
			
	
	Entropy 
		used to assess the amount of uncertainty in an outcome
			
		Measure the predictability of randomly selected objects
		
		>Capture amount of "surprise" in random selection
		
		>>>Formunla
		Given feature X, H(X) is its entropy
		
			Look at the proportion of items in the bins
			
			apply to each bin
				probability of each bin/type
				
		measured in bits
		
		High diversity of data
			High entropy
		
		Low diversity of data 
			Low entropy
			
		>Using log 2 would measure the uncertainty in bits
		
		
		
	Conditional Entropy 
		
		Measures how much information is needed to describe outcome Y, if outcome X is known
		
		Uncertainty of unknown outcome
		
		
		>>>Given that it is property X, what is the probability of property Y
		
		
		H(Y|X)--- Entropy of Y, given X
		
		>X can give information about Y
			Correlation
			
		

Mutual Information Definition
	MI(X,Y) =
				H(Y) - H(Y|X)
				H(X) - H(X|Y)
				
	
	How much information one gives about the other
	
	
	Large Value = Highly correlated
	
	Small Value = Low correlation
	
	given than MI(X,Y) > 0
		no upper bound
		
		normalised mutual information (NMI)
			NMI(X,Y) = MI(X,Y)/min(H(X),H(Y))
			
			create upper bound
			
	>>>Able to detect non-linear correlation
	
	Advantage
		Linear and Non-linear features
		features are discrete
		
	Disadvantages 
		If continuous data, needs to be discretised
		
		
		
			












































