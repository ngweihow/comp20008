Classifying Methodologies Cont.

	Best split
		Calculate the entropy of the children using the formula
		
		Measuring the diversity of the nodes
		
		Split unity
			Information Gain
			Entropy started minus Entropy gained
			
			Weighted Average
			
			>Formula
				Entropy(root) - Entropy(root|split)
				
			Purity of children
			
			>>>Information Gain value of 1 =  best split
				Purest and no ambiguity
		

		
	Creating Decision Tree
		Choosing attribute to split by
		Calculate the information gain for each possibility and find the highest one
		Goes down recursively
		
		
	Decision Tree
		Advantage
			Easy to use, walk down tree
			Easy to construct
			Fast for decisions
		Disadvantage
			Greedy construction strategy
				A lot of if rules
				Might let a lot of exceptions go through
				
			Maybe behave strangely
			Look at everything in training data set






























